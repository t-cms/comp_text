---
title: "Final Project"
author: "Tiasia Saunders"
date: "2025-05-01"
output: html_document
---

## Data Source

The original articles were retrieved from [Nexis Uni](https://www.lexisnexis.com/en-us/professional/academic/nexis-uni.page).

üëâ [Link to articles.csv on Google Drive](https://drive.google.com/drive/folders/1lYh1Ya3htdd24Yre1LakjNi3La3eUhS1?dmr=1&ec=wgc-drive-hero-goto)

## Description of Data

The dataset includes articles published in the first month of Trump's 2025 presidency (Jan2025-Feb2025) containing keywords related to DEI and the Trump administration. It includes variables like publication name, publication country, date, full article text, and a binary variable that I added to indicate whether the article is **domestic** or **international**. 

## Content Analysis Plan

I plan to conduct a content analysis to compare how DEI-related issues were framed across domestic and international newspapers. I‚Äôve created a **preliminary codebook** in my topic_modeling assignment to identify topic themes such as:

- **Trump Executive Orders**
- **Corporate DEI**
- **Federal Education Policy**
- **Airplane Helicopter Crash**
- **Trump Elon Muske**
- **Media Website Info** <- considered this not useful in my analysis


| **Topic Code**              | **Label**                 | **Definition / Description**                                                                                      | **Key Terms / Keywords Detected**                                                                 | **Included in Analysis?**              |
| --------------------------- | ------------------------- | ----------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- | -------------------------------------- |
| `trump_executive_orders`    | Trump Executive Orders    | Articles focused on Trump‚Äôs federal executive actions or policies concerning DEI (Diversity, Equity, Inclusion).  | ‚Äútrump‚Äù, ‚Äúorder‚Äù, ‚Äúexecut‚Äù, ‚Äúfeder‚Äù, ‚Äúpresid‚Äù, ‚Äúdivers‚Äù, ‚Äúdonald‚Äù, ‚Äúdei‚Äù, ‚Äúgovern‚Äù, ‚Äúinclus‚Äù      | ‚úÖ Yes                                  |
| `corporate_dei`             | Corporate DEI             | Coverage of corporate diversity programs, policies, or initiatives, especially those being criticized or removed. | ‚Äúdivers‚Äù, ‚Äúdei‚Äù, ‚Äúinclus‚Äù, ‚Äúequiti‚Äù, ‚Äúcompani‚Äù, ‚Äúpolici‚Äù, ‚Äútarget‚Äù, ‚Äúiniti‚Äù, ‚Äúback‚Äù, ‚Äúprogram‚Äù    | ‚úÖ Yes                                  |
| `federal_education_policy`  | Federal Education Policy  | DEI-related policies in the education sector, including federal funding or university programs.                   | ‚Äúfeder‚Äù, ‚Äúfund‚Äù, ‚Äútrump‚Äù, ‚Äúeduc‚Äù, ‚Äúunivers‚Äù, ‚Äústate‚Äù, ‚Äúprogram‚Äù, ‚Äúorder‚Äù, ‚Äústudent‚Äù, ‚Äúgrant‚Äù      | ‚úÖ Yes                                  |
| `airplane_helicopter_crash` | Airplane/Helicopter Crash | Articles incidentally mentioning DEI during unrelated news events such as aircraft crashes.                       | ‚Äútrump‚Äù, ‚Äúcrash‚Äù, ‚Äúplane‚Äù, ‚Äúwashington‚Äù, ‚Äúpresid‚Äù, ‚Äúdivers‚Äù, ‚Äúair‚Äù, ‚Äúblame‚Äù, ‚Äúhelicopt‚Äù, ‚Äúdonald‚Äù | ‚ö†Ô∏è Partial (exclude or tag separately) |
| `trump_elon_musk`           | Trump and Elon Musk       | Broader commentary pieces linking Trump and Elon Musk in DEI or political discussions.                            | ‚Äútrump‚Äù, ‚Äúpresid‚Äù, ‚Äúdonald‚Äù, ‚Äúmusk‚Äù, ‚Äúgovern‚Äù, ‚Äúworld‚Äù, ‚Äúhistori‚Äù, ‚Äúmonth‚Äù, ‚Äúday‚Äù, ‚Äúamerican‚Äù     | ‚úÖ Yes                                  |
| `media_website_info`        | Media Website Info        | Non-substantive content such as navigation menus or metadata‚Äîirrelevant to analysis.                              | ‚Äú‚Ä¢ news‚Äù, ‚Äúart‚Äù, ‚Äúsport‚Äù, ‚Äúsearch‚Äù, ‚Äúopinion‚Äù, ‚Äúcontent‚Äù, ‚Äústaff‚Äù, ‚Äúsubmit‚Äù, ‚Äúwashington‚Äù         | ‚ùå No (Exclude)                         |
| `other`                     | Other / Uncategorized     | Articles that do not fit into the defined categories.                                                             | N/A                                                                                               | ‚ö†Ô∏è Considered if useful                |


These categories are being developed based on the most frequent bigrams and common narrative themes. Preliminary coding involves using keyword matching and bigram frequency analysis to map patterns of coverage.

## Unit of Analysis

The unit of analysis is an individual news article. The content analysis will focus on examining several components within each article:

- The **publication_location column**
- The **dom_vs_intl column**
- The **tokenization/bigrams**
- The **sentiment analysis**
- The **topic modeling**



## DEI_Articles (my dataframe) Codebook: 

| Variable Name          | Description                                                               | Type      | Example                         |
| ---------------------- | ------------------------------------------------------------------------- | --------- | ------------------------------- |
| `V1`                   | Article index number (row ID)                                             | Numeric   | 1, 958, 1916                    |
| `title`                | Title of the article                                                      | Character | "Trump attacks DEI programs"    |
| `published_date`       | Date when the article was published                                       | Date      | 2025-01-28                      |
| `publication_location` | Geographic location of the news outlet (e.g., country, city)              | Character | "New York, USA"                 |
| `publication_4`        | News outlet name (alternate or duplicate of another publication variable) | Character | "New York Times"                |
| `publication_type_5`   | Type of publication (e.g., newspaper, magazine, wire service)             | Character | "Newspaper", "Wire"             |
| `length`               | Length of article in characters                                           | Numeric   | 690.5                           |
| `section`              | Section of the newspaper or outlet (e.g., Politics, Opinion)              | Character | "Politics", "Business"          |
| `word_count`           | Number of words in the article                                            | Numeric   | 829                             |
| `countries`            | Countries mentioned in the article                                        | Character | "United States, China"          |
| `byline`               | Author(s) of the article                                                  | Character | "By Jane Doe"                   |
| `agg_copyright`        | Aggregated copyright info (not used; all values = NA)                     | Logical   | NA                              |
| `cite`                 | Citation or source identifier                                             | Character | "NYT-123456"                    |
| `company`              | Parent media company                                                      | Character | "Gannett", "News Corp"          |
| `headline`             | Headline as printed in the publication                                    | Character | "Trump's DEI Agenda"            |
| `hlead`                | Headline lead/summary sentence                                            | Character | "Former president escalates..." |
| `publication_16`       | Possibly a duplicate of `publication_4`                                   | Character | "The Guardian"                  |
| `publication_type_17`  | Possibly a duplicate of `publication_type_5`                              | Character | "Online", "Print"               |
| `pub_copyright`        | Copyright info (not used; all values = NA)                                | Logical   | NA                              |
| `show`                 | Possibly denotes whether the article is from a show or broadcast (unused) | Logical   | NA                              |
| `term`                 | Possibly indicates search term used (all values = NA)                     | Logical   | NA                              |
| `ticker`               | Possibly stock ticker (unused)                                            | Logical   | NA                              |
| `index`                | Article index or tag                                                      | Character | "trump\_dei\_001"               |
| `filename`             | Name of the file where article is stored                                  | Character | "nyt\_2024\_01\_23\_trump.txt"  |
| `filepath`             | Full path to the article file                                             | Character | "/Users/tiasia/articles/..."    |


### Analysis Codebook 

| Code                | Definition                                                             | Example                                             |
|---------------------|------------------------------------------------------------------------|-----------------------------------------------------|
| Tone_Positive       | Article uses language reflecting hope, resolution, or positive outcomes | ‚ÄúProtesters call for reform, urge peaceful change‚Äù  |
| Tone_Negative       | Article uses alarmist or fear-inducing language                        | ‚ÄúMob storms the Capitol in unprecedented violence‚Äù  |
| Dom_vs_intl         | Classifies the article as Domestic or International                    | Domestic (e.g. NYT), International (e.g. BBC)       |
| Political_Affiliation | Implies alignment with a political ideology or party                | ‚ÄúLeft-wing activists confront right-wing group‚Äù     |

> _Note: Codebook is subject to refinement following further testing._

# Analysis Work 


## Load Libraries


```{r}
library(tidyverse)
library(janitor)
library(tidytext)
library(lubridate)
library(stringr)
library(ggplot2)
library(topicmodels)
library(lda)
library(tm)
library(knitr)
library(kableExtra)
library(reshape2)
library(tidyr)
library(dplyr)
```


### Compiled Index 

```{r}
index_1 <- rio::import("/Users/tiasiasaunders/Desktop/code/comp_text/DEI_updated_files/Results list for_trump_and_dei_1-999.XLSX")
index_2 <- rio::import("/Users/tiasiasaunders/Desktop/code/comp_text/DEI_updated_files/Results list for_trump and dei_1000-1999.XLSX")
index_3 <- rio::import("/Users/tiasiasaunders/Desktop/code/comp_text/DEI_updated_files/Results list for_trump and dei_2000-2999.XLSX")
index_4 <- rio::import("/Users/tiasiasaunders/Desktop/code/comp_text/DEI_updated_files/Results list for_trump_and_dei_3000-3999.XLSX")
index_5 <- rio ::import("/Users/tiasiasaunders/Desktop/code/comp_text/DEI_updated_files/Results list for_trump_and_dei_4000-4023.XLSX")
final_index <- rbind(index_1, index_2, index_3, index_4, index_5) |> clean_names()

rio::export(final_index, "/Users/tiasiasaunders/Desktop/code/comp_text/DEI_updated_files/final_index.xlsx")


## donwload last 23 articles make index_5 and join it in rbind 
```


[copy text compiler comp text code, rewrite code for my stuff in there ]

# Raw text compiler
```{r include=FALSE}
#This creates an index with the file path to the stories. And then it compiles the stories into a dataframe
#####################
# Begin SM Code #####
#####################
###
# List out text files that match pattern .txt, create DF
###
#Adjust thisline for your file name
files <- list.files("./file_raw_text", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an matching file name
  mutate(index = str_replace_all(filename, ".txt", "")) %>%
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", index))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25)) |> 
  distinct(index, .keep_all = TRUE)
#Join the file list to the index
final_data <- rio::import("/Users/tiasiasaunders/Desktop/code/comp_text/DEI_updated_files/final_index.xlsx") |> 
  clean_names() |> 
   #create an matching file name
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25)) |> 
  distinct(index, .keep_all = TRUE)
```




### Check for duplicate entries
```{r}
final_data |> 
  count(title) |> 
  arrange(desc(n))

## no duplicates in the final_data right now, but only have 3519 records so I'm missing 438 records compared to final index and 504 from the inital 4023 records downloaded from Nexis uni 
```


```{r}
dupe_data <- rio::import("/Users/tiasiasaunders/Desktop/code/comp_text/DEI_updated_files/final_index.xlsx") |> 
  clean_names() |> 
   #create an matching file name
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25))
```

```{r}
report <- dupe_data |> 
  count(title) |> 
  arrange(desc(n)) |> 
  filter(n > 1) |> 
  select(n, everything())
```

```{r}
final_index <- final_data |> 
  inner_join(files, c("index")) |> 
#you need the actual hard-coded path on this line below to the text
  mutate(filepath = paste0("./file_raw_text/", filename))
head(final_index)
```

#Fact Check
```{r}
anti_final_index <- final_data |> 
  anti_join(files, c("index"))
```

#Checking for duplicates
```{r}
final_index |> 
  count(title) |> 
  arrange(desc(n))
```


#Text compiler
```{r}
###
# Define function to loop through each text file 
###
create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)
# write.csv(articles_df, "../exercises/assets/extracted_text/kemi_df2.csv")
```


```{r}
 articles_df <- read_csv("/Users/tiasiasaunders/Desktop/code/comp_text/articles_df.csv")
```

```{r}
articles_df |> 
  distinct(title)

## use this for textual analysis (bigrams, tokenization, sentiment analysis) 
```

```{r}
articles_df <- articles_df |> 
mutate(dom_vs_intl = case_when (
  publication_location == "International" ~ "INTL",
  publication_location != "International" ~ "US",
  TRUE ~ NA
)) 

```


```{r}
final_index |> 
  count(publication_4)

final_data |> 
  count(publication_4)

```



## Publication Count 

```{r}
#filtered for 10mor more articles, 30 publications and 1479	 articles
pub_count <- final_index |> 
  count(publication_4) |> 
  filter( n> 10) %>% 
  arrange(desc(n)) 

print(pub_count)

### filter out publication to those who have 10 or more (already did ), mutate new column categorizing the 30 publicaations with centrist, left or right based on the media bias charts resources, add case_when 



```

- The top 5 publications are the University Wire, MailOnline, States News Service, CNN Transcripts and Targeted News Service. 


## Political Leaning Publication Assignment 
-add source for left, right, centrist, neutral 
```{r}

## redo this code block with the new data final_index 
final_index <- final_index |> 
  mutate(political_leaning = case_when(
    publication_4 %in% c("CNN Transcripts", "The Independent (United Kingdom)", "The Guardian (London)", "USA Today Online", "Axios", "The Associated Press", "CNN Wire", "The New York Times","USA Today") ~ "Left",
    publication_4 %in% c("MailOnline", "Fox News", "The Daily Caller", "telegraph.co.uk", "The Daily Telegraph (London)","The Times (London)" ) ~ "Right",
    publication_4 %in% c("Newsweek.com", "TheHill.com","CNN International", "Forbes.com (Forbes, Incorporated)", "Financial Times Online", "thetimes.co.uk", "Financial Times (London, England)") ~ "Centrist",
    publication_4 %in% c("University Wire","States News Service", "Targeted News Service", "Business Insider" ) ~ "Neutral", 
    TRUE ~ "Other"
  ))

```

```{r}
## getting a breakdown of the political leaning of the publications
political_leaning_count <- final_index |>
  count(political_leaning) |>
  mutate(
    total = sum(n),
    percent = n / total * 100
  )
## so how could i do a content analysis of right versus left versus centrist verus neutral 
```


## Number of columns and rows 

```{r}

print(paste0("This dataset has ", nrow(final_index), " rows and ", ncol(final_index), " columns."))

```

## Descriptive Statistics 

```{r}
summary(final_index)
```

### Adding categorizing columns such as dom_vs_intl to indicate whether the article is from an international or domestic outlet 
```{r}
final_index <- final_index |> 
 #mutating a new column to indicate whether the pub location is international or domestic 

mutate(dom_vs_intl = case_when (
  publication_location == "International" ~ "INTL",
  publication_location != "International" ~ "US",
  TRUE ~ NA
)) 


```

## Frequency count for each category and their location 

```{r}
table(final_index$dom_vs_intl)
table(final_index$publication_location)

```

## Top 5 Domestic Articles 
```{r}
top_five_domestic <- final_index|>
  filter(dom_vs_intl == "US") |>
  count(publication_4, sort = TRUE) |>
  slice_max(n, n = 5) 

print(top_five_domestic)


```


## Top 5 International Articles 
```{r}
top_five_intl <- final_index |>
  filter(dom_vs_intl == "INTL") |>
  count(publication_4, sort = TRUE) |>
  slice_max(n, n = 5) 
print(top_five_intl)


```


### GGplot of the articles over time: From the 01/20-02-20-- one month of Trump's 2025 presidency 

```{r}
df_time <- final_index |> 
  count(published_date, dom_vs_intl)

ggplot(df_time, aes(x = published_date, y = n, color = dom_vs_intl)) +
  geom_line(size = 1) +
  labs(
    title = "Domestic vs International Coverage Over Time",
    x = "Publication Date",
    y = "Number of Articles",
    color = "Outlet Type", 
    caption = ".\nGraphic by Tiasia Saunders (redacted - peer review)  05/11/2025"
  ) +
  theme_minimal()


```

### Graphic of Domestic versus International Outlet Distribution 
```{r}
ggplot(final_index, aes(x = dom_vs_intl)) +
  geom_bar(fill = "violet") +
  labs(
    title = "Distribution of Domestic vs International Coverage",
    x = "Outlet Type",
    y = "Number of Articles", 
      caption = ".\nGraphic by Tiasia Saunders (redacted - peer review)  05/11/2025"
  ) +
  
  theme_minimal()

```

## Tokenizing Data (start here tomorrow)

```{r}
stories <- str_replace_all(articles_df$hlead, "- ", "")
stories_df <- tibble(stories)

# unnest includes lower, punct removal

stories_tokenized <- stories_df |> 
  unnest_tokens(word,stories)

stories_tokenized

```

#### Remove stopwords and count the words

```{r}

stories_tokenized <- stories_tokenized |>
  anti_join(stop_words, by = c("word" = "word")) 

```

### Word Count

```{r}
story_word_ct <- stories_tokenized %>%
  count(word, sort=TRUE)
```

### Create Bigrams

```{r}
bigrams <- stories_df %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)

```


```{r}
##filtering bigrams to reduce the processing lug to 5 or more instances of bigrams 
bigram_counts <- bigrams %>%
  count(bigram) %>%
  filter(n >= 5) %>%
  arrange(desc(n))
```

```{r}
#Filter out stop words.

## having issues with this chunk its not running tried multiple times 

bigram_counts_1 <- bigram_counts |> 
  separate(bigram, c("word1", "word2"), sep = " ")
```

```{r}
filtered_bigram_count <- bigram_counts_1 |> 
  filter(!word1 %in% stop_words$word) |> 
  filter(!word2 %in% stop_words$word) |> 
  filter(!is.na(word1), !is.na(word2)) |> 
  mutate(bigram = str_c(word1, word2, sep = " ")) |> 
  filter(
    !str_detect(bigram, regex("news channel|cnn host|cnn anchor|news host|begin video|channel host|00 00", ignore_case = TRUE))
  )
```


```{r}
filtered_bigram_count <- bigram_counts_1 |> 
  filter(!word1 %in% stop_words$word) |> 
  filter(!word2 %in% stop_words$word) |> 
    filter(!is.na(word1)) |> 
     filter(!is.na(word2))


```


## Top twenty bigrams 
```{r}

top_bigrams <- filtered_bigram_count |>
  arrange(desc(n), word1, word2) |>
  head(20)


```

### Graphic of the Top 20 Most Frequent Biagrams 
```{r}
# Combine the bigram into one string for labeling
top_bigrams <- filtered_bigram_count |>
  arrange(desc(n), word1, word2) |>
  head(20) |>
  mutate(bigram = paste(word1, word2))

# Plot
ggplot(top_bigrams, aes(x = reorder(bigram, n), y = n)) +
  geom_col(fill = "lavender") +
  coord_flip() +
  labs(
    title = "Top 20 Most Frequent Bigrams",
    x = "Bigram",
    y = "Frequency", 
      caption = ".\nGraphic by Tiasia Saunders (redacted - peer review)  05/11/2025"
  ) +
  theme_minimal()

## filter out news channel, 00:00 (after looking at context), cnn host, cnn anchor, channel host, 
```

## Creating Domestic articles tokenization
```{r}
dom_dei_articles <- articles_df  |> 
  filter(dom_vs_intl == "US")


stories <- str_replace_all(dom_dei_articles$hlead, "- ", "")
dom_stories_df <- tibble(stories)

# unnest includes lower, punct removal

dom_stories_tokenized <- dom_stories_df %>%
  unnest_tokens(word,stories)

dom_stories_tokenized

dom_stories_tokenized <-dom_stories_tokenized |>
  anti_join(stop_words, by = c("word" = "word")) 

story_word_ct <- dom_stories_tokenized %>%
  count(word, sort=TRUE)

```

### Creating International articles tokenization 

```{r}
intl_dei_articles <- articles_df |> 
  filter(dom_vs_intl == "INTL")


stories <- str_replace_all(intl_dei_articles$hlead, "- ", "")
intl_stories_df <- tibble(stories)

# unnest includes lower, punct removal

intl_stories_tokenized <- intl_stories_df %>%
  unnest_tokens(word,stories)

intl_stories_tokenized

intl_stories_tokenized <- intl_stories_tokenized |>
  anti_join(stop_words, by = c("word" = "word")) 

story_word_ct <- intl_stories_tokenized %>%
  count(word, sort=TRUE)

```

## Sentiment Analysis 
```{r}
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")

```


#### Joining Domestic and Sentiment 

```{r}
sentiments_dom_all <- dom_stories_tokenized |>
  inner_join(nrc_sentiments) 
dom_overall_sentiment_count <- sentiments_dom_all |> 
  count(sentiment) |> 
  arrange(desc(n))

```

#### Joining International and Sentiment 
```{r}
sentiments_intl_all <- intl_stories_tokenized |>
  inner_join(nrc_sentiments) 
intl_overall_sentiment_count <- sentiments_intl_all |> 
  count(sentiment) |> 
  arrange(desc(n))


```

```{r}
# Add source labels
dom_overall_sentiment_count <- dom_overall_sentiment_count |> 
  mutate(source = "US")

intl_overall_sentiment_count <- intl_overall_sentiment_count |> 
  mutate(source = "INTL")

# Combine both dataframes
combined_sentiment <- bind_rows(dom_overall_sentiment_count, intl_overall_sentiment_count)

# Plot
ggplot(combined_sentiment, aes(x = sentiment, y = n, fill = source)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Sentiment Count: Domestic vs International Trump DEI Articles",
    x = "Sentiment",
    y = "Count",
    fill = "Article Source"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
 
sentiments_all <-stories_tokenized |> 
  inner_join(nrc_sentiments) 
```

### Overall Sentiment Count 
```{r}
overall_sentiment_count <- sentiments_all |> 
  count(sentiment) |> 
  arrange(desc(n))


```


## Graphic of Sentiment Analysis over time 

```{r}
ggplot(overall_sentiment_count, aes(x = reorder(sentiment, n), y = n, fill = sentiment)) +
  geom_bar(stat = "identity") + 
  coord_flip() +  
  labs(
    title = "Sentiment Distribution in Trump DEI Articles",
    x = "Sentiment",
    y = "Count",
    caption = "01/20‚Äì02/20.\nGraphic by Tiasia Saunders (redacted - peer review)  05/11/2025"
  ) +
  theme_minimal()

## lok into trust, fear and surprise 
```

### Leading postive sentiment 
```{r}
overall_postive_count <- sentiments_all |> 
   filter(sentiment == "positive") |> 
  count(sentiment,word) |> 
  arrange(desc(n))

print(overall_postive_count)

```

### Leading Negative Sentiment 
```{r}
overall_negative_count <- sentiments_all |> 
   filter(sentiment == "negative") |> 
  count(sentiment,word) |> 
  arrange(desc(n))

print(overall_negative_count)

```


### Getting the top twenty words for each sentiment
```{r}
# Get the top 20 words per sentiment
top_words_per_sentiment <- sentiments_all |> 
  group_by(sentiment, word) |> 
  summarise(count = n(), .groups = "drop") |> 
  arrange(sentiment, desc(count)) |> 
  group_by(sentiment) |> 
  slice_max(order_by = count, n = 20)
```

## Graphic of Sentiment Breakdown for Domestic Articles 
```{r}
sentiments_dom_all %>%
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%  
  ungroup() %>%
  mutate(word = reorder_within(word, n, sentiment)) %>%  
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +  
  scale_y_reordered() + 
  labs(
    x = "Sentiment Analysis Breakdown For Domestic Articles",
    y = NULL,
    caption = ".\nGraphic by Tiasia Saunders (redacted - peer review)  05/11/2025"
  ) 

## war, defense and military in fear sentiment 
```

## Graphic of Sentiment Breakdown for International Articles 
```{r}
sentiments_intl_all %>%
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%  
  ungroup() %>%
  mutate(word = reorder_within(word, n, sentiment)) %>%  
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +  
  scale_y_reordered() + 
  labs(
    x = "Sentiment Analysis Breakdown For International Articles",
    y = NULL,
    caption = ".\nGraphic by Tiasia Saunders (redacted - peer review)  05/11/2025"
  ) 
## look into the terms then look at the keyord 

```

## Topic Modeling

```{r}
textdata <- articles_df |>  
  
  select(filename, hlead, published_date, publication_location, dom_vs_intl) |> 
  as.data.frame() |> 
  rename(doc_id = filename, text= hlead)

# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
```

```{r tm3a}
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339

```

## Topic proportions over time {.unnumbered}

```{r}
# dont need this since i am not using dates 
textdata$day <- weekdays(as.Date(textdata$published_date, format="%Y-%m-%d"))
```

## Articles per international vs domestic newspapers 

```{r}
## Articles per international vs domestic newspapers 

articles_per_dom_intl_category <- textdata |> 
  distinct(doc_id, .keep_all=TRUE) |>  
  count(dom_vs_intl) |> 
  mutate(pct_total = (n / sum(n))) |> 
  mutate(pct_total = formattable::percent(pct_total)) |> 
  arrange(desc(dom_vs_intl))

articles_per_dom_intl_category %>%
  kbl(caption = "Trump DEI Articles Per Outlet Type", font_size = 30) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em") %>% 
  column_spec(3, width = "5em", background = "yellow")

sum(articles_per_dom_intl_category$n)
```


##### So I have 3504 total articles. In my dataframe US articles make up 74.91  percent and international artidcles make up 25.09 percent. 

```{r tm12}
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
```

### Mean topic proportions per domestic and international category 

```{r}
# Step 1: Check dimensions
# Ensure doc_id and rownames are character
rownames(theta) <- as.character(rownames(theta))
textdata$doc_id <- as.character(textdata$doc_id)

# Get common document IDs
common_ids <- intersect(rownames(theta), textdata$doc_id)

# Filter both datasets to only include common documents
theta_aligned <- theta[rownames(theta) %in% common_ids, ]
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]

# Reorder textdata_filtered to match theta_aligned
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]

# Check that everything lines up
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
  stop("The document IDs still do not match. Please check the data alignment.")
}

# Print dimensions
n_theta <- nrow(theta_aligned)
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of aligned theta rows: ", n_theta, "\n")
cat("Number of aligned textdata rows: ", n_textdata_filtered, "\n")

# Combine data
topic_data <- data.frame(theta_aligned, dom_vs_intl = textdata_filtered$dom_vs_intl)

# Aggregate topic proportions by dom_vs_intl
topic_proportion_per_category <- aggregate(. ~ dom_vs_intl, data = topic_data, FUN = mean)

# Rename topic columns (assuming K and topicNames are defined)
colnames(topic_proportion_per_category)[2:(K+1)] <- topicNames

# Reshape for visualization
vizDataFrame <- melt(topic_proportion_per_category, id.vars = "dom_vs_intl")

# Optionally filter out a decade if needed
# vizDataFrame <- subset(vizDataFrame, decade != 1960)
```


#### Examine topic names

```{r}
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>% 
  unnest(cols = c(text)) 
  
topics
```

```{r}
# Remove the specific unwanted topic
topics <- topics %>%
  filter(text != "‚Ä¢ news search art sport opinion student content submit contentclassskipcontentskip")
```

### Review the topics and determine a 1-2 word label after reading the source documents.

```{r}

#Topic 1	counti citi night mile jail day town morn march juli

theta2 <- as.data.frame(theta)

topic1 <- theta2 %>% 
  rownames_to_column(var = "file") |> # putting the rownames into a new column called file
  mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
         line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
  mutate(file = str_remove(file, "\\.\\d+$")) |> 
  rename(topic1 = '1') |> # looking at first topic: ounti citi night mile jail day town morn march juli
  top_n(20, topic1) |> 
  arrange(desc(topic1)) |>  
  select(file, line, topic1) 


```

```{r}

#add categories
vizDataFrame <- vizDataFrame |>
  mutate(category = case_when(
    str_detect(str_squish(variable), "trump order presid feder execut dei divers donald program administr") ~ "trump_executive_orders",
    str_detect(variable, "trump presid cnn musk air hous fire donald feder white") ~ "trump_elon_musk",
    str_detect(variable, "trump feder dei divers govern donald musk compani ‚Äô ‚Äú") ~ "federal_dei_policies",
    str_detect(variable, "trump news fox host presid democrat channel america doge interview") ~ "trump_fox_news",
    str_detect(variable, "crash trump collis plane helicopt air black washington dead nation") ~ "air_crash_trump",
    str_detect(variable, "‚Ä¢ news search art sport opinion student content submit contentclassskipcontentskip") ~ "misc_noise",
    TRUE ~ "other"
  ))
  

```

## Fact Check and Validate Topics

--Topic 1: 	‚Ä¢ news search art sport opinion student content submit contentclassskipcontentskip
--Topic 2: trump order presid feder execut dei divers donald program administr
--Topic 3: trump presid cnn musk air hous fire donald feder white
--Topic 4: trump feder dei divers govern donald musk compani ‚Äô ‚Äú
--Topic 5: trump news fox host presid democrat channel america doge interview
--Topic 6: crash trump collis plane helicopt air black washington dead nation


#### For Topic 2 dei_topics in administration topic

```{r}
# Assuming `theta` is a dataframe related to DEI topics
theta2 <- as.data.frame(theta)

# Extract and rename relevant column for DEI topic (e.g., for "female" or another DEI-related category)
dei_topic <- theta2 %>%
  rename(dei_topic = '2') %>%   # Replace '4' with the column that represents the DEI topic
  top_n(20, dei_topic) %>%
  arrange(desc(dei_topic)) %>% 
  select(dei_topic)

# Apply rownames_to_column to include story IDs
dei_topic <- tibble::rownames_to_column(dei_topic, "story_id")

# Clean up story_id if necessary (e.g., removing "X" or other unwanted characters)
dei_topic$story_id <- gsub("X", "", dei_topic$story_id)

# Check the top 20 story IDs
head(dei_topic$story_id, 20)
# Now it‚Äôs validated for DEI topics



```

#### For dei_topics in in topic 1 university 

```{r}
theta2 <- as.data.frame(theta)

university <- theta2 %>% 
  #renaming for a general topic
  rename(university = '1') %>% 
  top_n(20, university ) %>%
  arrange(desc(university )) %>% 
  select(university)

# Apply rownames_to_column
university  <- tibble::rownames_to_column(university , "story_id") 

university $story_id <- gsub("X", "", university $story_id)

head(university$story_id, 20)
#Checks out


```

#### For dei_topics in topic 3 Trump/Elon musk 

```{r}
theta2 <- as.data.frame(theta)

trump_elon <- theta2 %>% 
  #renaming for a general topic
  rename(trump_elon = '3') %>% 
  top_n(20,trump_elon ) %>%
  arrange(desc(trump_elon )) %>% 
  select(trump_elon)

# Apply rownames_to_column
trump_elon <- tibble::rownames_to_column(trump_elon , "story_id") 

trump_elon $story_id <- gsub("X", "", trump_elon $story_id)

head(trump_elon$story_id, 20)
#Checks out 
#main theme is negros are lynching victims

```

#### For dei_topics in topic 5 corporate_dei_policies

```{r}
theta2 <- as.data.frame(theta)

# Rename and extract top 20
corporate_dei_policies <- theta2 %>%
  rename(corporate_dei_policies = '5') %>%
  top_n(20, corporate_dei_policies) %>%
  arrange(desc(corporate_dei_policies))

# Move rownames to a column BEFORE selecting
corporate_dei_policies <- tibble::rownames_to_column(corporate_dei_policies, "story_id")

# Clean up the story_id column
corporate_dei_policies$story_id <- gsub("X", "", corporate_dei_policies$story_id)

# Optionally, select only the columns you want now
corporate_dei_policies <- corporate_dei_policies %>%
  select(story_id, corporate_dei_policies)
#Checks out 


```

## Topic Modeling Visualization (make it a split of mini graphics of the topic modeling subject with the words)
```{r}
ggplot(vizDataFrame, aes(x = dom_vs_intl, y = value, fill = category)) + 
  geom_bar(stat = "identity") +
  ylab("Proportion") + 
  scale_fill_manual(
    values = c("#9933FF", "#33FFFF", "red", "yellow", "darkblue", "brown" ),
    name = "Dei Topics"
  ) + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1)
  ) +
  labs(
    title = "Common Narratives in Trump DEI Coverage",
    subtitle = "Six probable topics in domestic and international media coverage sample. n = 1,916",
    caption = "Aggregate mean topic proportions per day from 01/20‚Äì02/20.\nGraphic by Tiasia Saunders  09/13/2025"
  )

```

## Continued Plan: 

- To take a good look at seniment analysis and possibly implore new method that Rob sent me. 
- To look deeper into political leaning of the publications and see if there is any trends there.
- Will attempt to do a topic modeling for US and International articles depending on the political     affiliation of the publication.
- To develop more graphics for the topic modeling section
- To figure out the insights from the additional sentiment analysis 
- To manually look through data and provide context examples for paper. 



## My Findings: 

###Key Findings from Overall Analysis: 
- international articles topics focused more on federal_dei_policies and air_crash_trump
where as domestic articles topics focuse more on trump_elon_musk and trump_fox_news
-the top 20 bigrams included: diversity equity, white house,  donald trump, president dei which is not surprising, other ones included, history month, civil rights.  
-topic modeling: Overall, the most interesting topics for me were the **trump_exective_orders, federal_education_policy, trump_elon_musk, airline_helicopter_crash (because Trump attributed it to DEI hires failure).**


### Comparing International vs. Domestic Newspaper Coverage of DEI Issues under Trump: A Content Analysis

In this project, I conducted a content analysis of 3,504 news articles that mentioned DEI-related (Diversity, Equity, and Inclusion) issues during the post-affirmative action landscape, with a special focus on former President Donald Trump‚Äôs policies. My goal was to explore how coverage of these issues varied between **domestic** (U.S.-based) and **international** media outlets. Through a combination of topic modeling, bigram analysis, and sentiment analysis, I aimed to uncover the thematic, emotional, and topical patterns shaping news discourse on DEI during this period.

### My Limitations 
- I downloaded 4,023 articles from NexisUni, upon loading them in R I had 519 duplicate entries which I deleted from the data. So, in the end I have 3,504 articles. 
- I am  analyzing articles published between January 20, 2025 (the start of Trump‚Äôs second term) and February 28, 2025. This one-month window may not fully capture longer-term trends or shifts in DEI coverage.
- The dataset is limited to articles available through NexisUni, which may not represent the full spectrum of media outlets or perspectives on DEI issues.
- I manually selected the publications, so I could have a wide range of politically leaning outlets, but there may still be selection bias in which outlets were included.



#### Top Bigrams Reveal Central Framing

My analysis of the top 20 bigrams (two-word combinations) in the dataset provided further insight into how DEI-related discourse is framed. Among the most common bigrams were:

* **‚ÄúDonald Trump"‚Äù**
* **‚ÄúWhite house‚Äù**
* **‚ÄúPresident Trump‚Äù**
* **‚ÄúFox News‚Äù**
* **‚ÄúPresident Donald‚Äù**
* **‚ÄúElon Musk‚Äù**


These pairings confirm that discussions of DEI were closely tied to federal leadership and political developments. Terms like ‚ÄúWhite House‚Äù and ‚ÄúDonald Trump‚Äù suggest a central focus on presidential authority, while ‚Äúdiversity equity‚Äù and ‚Äúcivil rights‚Äù speak to the broader social justice context. The inclusion of ‚Äúhistory month‚Äù likely reflects how DEI is seasonally spotlighted during observances like Black History Month‚Äîfurther evidence that coverage may be reactive or symbolic rather than ongoing or systemic.



#### Sentiment Analysis: Fear, Anger, and Sadness 

Emotionally, the media coverage of DEI under Trump was far from neutral. A sentiment analysis of the article corpus revealed high counts of negative affect:

* **Positive** appeared **2742253** times 
* **Trust** appeared **1992543** times 
* **Fear** appeared **1334435** times
* **Anger** appeared **956269** times
* **Sadness** appeared **746093** times


##### Postive Leading Words: 

Overall the top positive words such as president, equity, content, and united made sense; however as I looked on words like basketball, police, widespread, and choice could be perceived as negative or neutral in some context. 
### Leading negative sentiment 

##### Negative Leading Words:

As for the negative words, words such as war, ban, force, and collision fit perfectly in my opinion. Despite this, other words such as skip, tax, serve, case, and spoke could have been neautral or negative in some context. 

#### Sentiment Analysis: Domestic versus International 

These results were surprising and suggest that DEI coverage during this time was emotionally charged. Fear may stem from concerns about the rollback of protections or the uncertainty of federal policy. Anger likely reflects opposition to perceived injustices or backlash against diversity efforts. Sadness could correspond with a mourning of social progress or the erasure of inclusive policies. Taken together, these emotions paint a picture of a politically and emotionally volatile media environment around DEI.

This emotional charge could either amplify or dampen public engagement, depending on how readers interpret and internalize these emotions. Media coverage filled with fear and anger can spur action‚Äîbut it can also result in emotional burnout and disengagement.

#### Topic Modeling: Identifying Thematic Clusters

To better understand the underlying themes across the dataset, I applied topic modeling using keyword-based string detection. From this, I coded articles into six main topic categories:

1. **Trump Executive Orders** ‚Äì Articles discussing specific policy rollbacks and federal mandates that banned or restricted DEI training and language in government agencies.
2. **Trump Fox News** ‚Äì Coverage emphasizing Fox News reporting on companies‚Äô diversity initiatives, political commentary, and internal corporate culture debates.
3. **Federal DEI Policies** ‚Äì Articles dealing with the intersection of DEI and higher education, including student grants, Title IX issues, and university policy changes.
4. **Airplane/Helicopter Crash** ‚Äì A non-DEI-related outlier category that likely emerged due to Trump being mentioned in unrelated breaking news contexts.
5. **Trump and Elon Musk** ‚Äì Articles speculating on shared ideologies or public commentary by the two figures, often linked to anti-DEI sentiment.
6. **Misc/Noise** ‚Äì Flagged as **not useful**, this group contained metadata and generic web content rather than editorial substance. (Ended up filtering this topic out of the topic modeling visualization and findings result). 


#### Domestic vs. International Framing

[will rewrite the section below upon additional data analysis]

When comparing topic categories across domestic and international coverage, early results suggest **international articles tended to frame DEI as part of a broader human rights narrative**, linking U.S. policy to global democratic trends or regressions. Domestic outlets, in contrast, were more likely to treat DEI as a political football, emphasizing partisan divides or executive overreach.

This framing distinction has important implications. It suggests that **international media may provide a more structural or global view**, whereas domestic outlets focus more narrowly on short-term political developments. This difference could influence how readers in each region understand the stakes and significance of DEI issues. 


